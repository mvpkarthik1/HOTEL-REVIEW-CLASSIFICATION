# -*- coding: utf-8 -*-
"""HOTEL REVIEW CLASSIFICATION P64 GROUP 6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1reLbt6TUep7XtVMRPXz9iFa1ctKp9zZv
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np 
import string 
import spacy
import seaborn as sns
from matplotlib.pyplot import imread
from matplotlib import pyplot as plt
from wordcloud import WordCloud
# %matplotlib inline
#import plotly as py
#import cufflinks as cf
#from plotly.offline import iplot
from nltk.corpus import stopwords
from textblob import TextBlob
from textblob import Word
import nltk
nltk.download('stopwords')
nltk.download('wordnet')

#!pip install nltk

#!pip install pandas

#py.offline.init_notebook_mode(connected=True)
#cf.go_offline()

import pandas as pd
reviews_df = pd.read_csv('Reviews.csv')
print(reviews_df)
print("Total Reviwes Extracted:", len(reviews_df))

data=reviews_df.rename({'REVIEWS':'reviews'},axis=1 )
data.head()

data.info()

stop = stopwords.words('english')
#data = data.head(500)
data['stopwords'] = data['reviews'].apply(lambda x: len([x for x in x.split() if x in stop]))
#data[['reviews','stopwords']].head(12)

data['stopwords'].sum()

corpus=[]
df= data['reviews'].str.split()
df=df.values.tolist()
corpus=[word for x in df for word in x]

from collections import defaultdict
dic=defaultdict(int)
for word in corpus:
    if word in stop:
        dic[word]+=1

top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] 
x,y=zip(*top)
plt.bar(x,y)

"""# We can evidently see that stopwords such as “the”, and” and “to” dominate in reviews."""

#Number of Words in single review
data['word_count'] = data['reviews'].apply(lambda x: len(str(x).split(" ")))
#data[['reviews','word_count']].head()

#Number of characters in single review including spaces
data['char_count'] = data['reviews'].str.len() 
#data[['reviews','char_count']].head()

def get_avg_word_len(x):
    words=x.split()
    word_len=0
    for word in words:
        word_len=word_len+len(word)
        
    return word_len/len(word)

data['avg_word_len']=data['reviews'].apply(lambda x:get_avg_word_len(x))
#data[['reviews','avg_word_len']].head()

data.head()

#replacing special characters with " "
data['reviews'] = data['reviews'].str.replace('[^\w\s\'\"]','')
data['reviews'].head()

#Lowercasing
data['reviews'] = data['reviews'].apply(lambda x: " ".join(x.lower() for x in x.split()))
data['reviews'].head()

#Removing stopwords
sw = stopwords.words('english')
data['reviews'] =data['reviews'].apply(lambda x: " ".join(x for x in x.split() if x not in sw))
data['reviews'].head()

#tokenization
nltk.download('punkt')

hotel_review=np.array(data['reviews'])
hr=str(hotel_review)
hr

from nltk.tokenize import word_tokenize
text_tokens = word_tokenize(hr)
print(text_tokens[:500])

#lemmatization
data['reviews'] = data['reviews'].apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))
data['reviews'].head()

data.head()

#for removing undesirable words with higher frequency
list=("hotel","us","taj","mumbai","india","mahal","would","every","u","made","palace")
data['reviews'] =data['reviews'].apply(lambda x: " ".join(x for x in x.split() if x not in list))
data['reviews'].head()

from collections import Counter
def plot_top_non_stopwords_barchart(x):
    stop=set(stopwords.words('english'))
    
    new= data['reviews'].str.split()
    new=new.values.tolist()
    corpus=[word for i in new for word in i]

    counter=Counter(corpus)
    most=counter.most_common()
    x, y=[], []
    for word,count in most[:25]:
        if (word not in stop):
            x.append(word)
            y.append(count)
            
    sns.barplot(x=y,y=x)

plot_top_non_stopwords_barchart(data['reviews'])

# Joining the list into one string/text
text = ' '.join(data['reviews'])
#text

# Commented out IPython magic to ensure Python compatibility.
# Import packages
import matplotlib.pyplot as plt
# %matplotlib inline
from wordcloud import WordCloud, STOPWORDS
# Define a function to plot word cloud
def plot_cloud(wordcloud):
    # Set figure size
    plt.figure(figsize=(40, 30))
    # Display image
    plt.imshow(wordcloud) 
    # No axis details
    plt.axis("off");

# Generate wordcloud
stopwords = STOPWORDS
stopwords.add('will')
wordcloud = WordCloud(width = 3000, height = 2000, background_color='black', max_words=100,colormap='Set2',stopwords=stopwords).generate(text)
# Plot
plot_cloud(wordcloud)

#sentiment analysis
data['sentiment_polarity'] = data['reviews'].apply(lambda x: TextBlob(x).sentiment.polarity )
data[['reviews','sentiment_polarity']].head(10)

def getAnalysis(score):
  if score < 0:
    return 'Negative'
  elif score == 0:
    return 'Neutral'
  else:
    return 'Positive'
data ['sentiment'] = data  ['sentiment_polarity'].apply(getAnalysis )
data[['reviews','sentiment','sentiment_polarity']].head(20)

count=data['sentiment'].value_counts()
count

import matplotlib.pyplot as plt

fig=plt.figure(figsize=(10,5))

senti=['Positive','Negative','Neutral']
plt.bar(senti,count,color='g')

"""# Feature Extractiion

## count vectoriser tells the frequency of a word.
"""

#!pip install sklearn

# count vectoriser tells the frequency of a word.
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
vectorizer = CountVectorizer(min_df = 1, max_df = 0.9)
X = vectorizer.fit_transform(data["reviews"])
word_freq_df = pd.DataFrame({'reviews': vectorizer.get_feature_names(), 'occurrences':np.asarray(X.sum(axis=0)).ravel().tolist()})
word_freq_df['frequency'] = word_freq_df['occurrences']/np.sum(word_freq_df['occurrences'])
#print(word_freq_df.sort('occurrences',ascending = False).head())

word_freq_df

"""## TFIDF - Term frequency inverse Document Frequencyt"""

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000, max_df = 0.5, smooth_idf=True) #keep top 1000 words
doc_vec = vectorizer.fit_transform(data["reviews"])
names_features = vectorizer.get_feature_names()
dense = doc_vec.todense()
denselist = dense.tolist()
df = pd.DataFrame(denselist, columns = names_features)

df

"""# Word2Vec"""

#!pip install python-Levenshtein

import gensim

#!pip install gensim

review_text = data.reviews.apply(gensim.utils.simple_preprocess)

review_text

model_w2v = gensim.models.Word2Vec(
    window=10,
    min_count=2,
    workers=4,
)

model_w2v.build_vocab(review_text, progress_per=1000)

model_w2v.train(review_text, total_examples=model_w2v.corpus_count, epochs=model_w2v.epochs)

#model.wv.most_similar("luxury")

#model.wv.similarity(w1="luxury", w2="expensive")

#model.wv.similarity(w1="excellent", w2="service")

"""# N-gram"""

#Bi-gram
def get_top_n2_words(corpus, n=None):
    vec1 = CountVectorizer(ngram_range=(2,2),  #for tri-gram, put ngram_range=(3,3)
            max_features=2000).fit(corpus)
    bag_of_words = vec1.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in     
                  vec1.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], 
                reverse=True)
    return words_freq[:n]

top2_words = get_top_n2_words(data["reviews"], n=200) #top 200
top2_df = pd.DataFrame(top2_words)
top2_df.columns=["Bi-gram", "Freq"]
top2_df.head()

#Bi-gram plot
import matplotlib.pyplot as plt
import seaborn as sns
top20_bigram = top2_df.iloc[0:20,:]
fig = plt.figure(figsize = (10, 5))
plot=sns.barplot(x=top20_bigram["Bi-gram"],y=top20_bigram["Freq"])
plot.set_xticklabels(rotation=45,labels = top20_bigram["Bi-gram"])

#Tri-gram
def get_top_n3_words(corpus, n=None):
    vec1 = CountVectorizer(ngram_range=(3,3), 
           max_features=2000).fit(corpus)
    bag_of_words = vec1.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in     
                  vec1.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], 
                reverse=True)
    return words_freq[:n]

top3_words = get_top_n3_words(data["reviews"], n=200)
top3_df = pd.DataFrame(top3_words)
top3_df.columns=["Tri-gram", "Freq"]

top3_df

#Tri-gram plot
import seaborn as sns
top20_trigram = top3_df.iloc[0:20,:]
fig = plt.figure(figsize = (10, 5))
plot=sns.barplot(x=top20_trigram["Tri-gram"],y=top20_trigram["Freq"])
plot.set_xticklabels(rotation=45,labels = top20_trigram["Tri-gram"])

"""# TFIDF"""

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(data['reviews'])

from sklearn import preprocessing
# label_encoder object knows how to understand word labels. 
label_encoder = preprocessing.LabelEncoder()
# Encode labels in column 'Country'. 
Y  = label_encoder.fit_transform(data['sentiment'])

X

Y

from imblearn.over_sampling import SMOTE
sm = SMOTE(k_neighbors=1)
X_res, Y_res = sm.fit_resample(X,Y)
print("After Oversampling the shape of X_train:{}".format(X_res.shape))
print("After Oversampling the shape of y_train: {} \n".format(Y_res.shape))

import seaborn as sns
sns.set_style("whitegrid")
sns.countplot(Y_res, palette = "hls")

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X_res, Y_res, test_size=0.3, random_state=100)

X_train

X_test

Y_train

Y_test

#!pip install imblearn

"""# NAIVE BAYES"""

from sklearn.naive_bayes import MultinomialNB as MNB
# Multinomial Naive Bayes
classifier_mnb = MNB()
classifier_mnb.fit(X_train,Y_train)

Y_pred_m = classifier_mnb.predict(X_test)
accuracy_m = np.mean(Y_test)

print(accuracy_m)

from sklearn.metrics import classification_report
print(classification_report(Y_test, Y_pred_m))

from sklearn.naive_bayes import GaussianNB as GNB
# Multinomial Naive Bayes
classifier_gnb = GNB()
classifier_gnb.fit(X_train.toarray(),Y_train)

Y_pred_g = classifier_gnb.predict(X_test.toarray())
accuracy_g = np.mean(Y_test)

print(accuracy_g)

from sklearn.metrics import classification_report
print(classification_report(Y_test, Y_pred_g))

"""# KNN CLASSIFICATION"""

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report,multilabel_confusion_matrix

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt 
# %matplotlib inline
# choose k between 2 to 41
k_range = range(2, 40,2)
k_scores = []
# use iteration to caclulator different k in models, then return the average accuracy based on the cross validation
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, Y_train, cv=10)
    k_scores.append(scores.mean())
# plot to see clearly
plt.plot(k_range, k_scores)
plt.xlabel('Value of K for KNN')
plt.ylabel('Cross-Validated Accuracy')
plt.show()

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt 
# %matplotlib inline
# choose k between 1 to 41
k_range = range(2, 40,2)
k_scores_train = []
k_scores_test = []
# use iteration to caclulator different k in models, then return the average accuracy based on the cross validation
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train,Y_train)
    pred_train= knn.predict(X_train)
    score = np.mean(pred_train)
    k_scores_train.append(score)
    pred_test = knn.predict(X_test)
    score_test = np.mean(pred_test)
    k_scores_test.append(score_test)
# plot to see clearly
plt.plot(k_range, k_scores_train)
plt.plot(k_range, k_scores_test)
plt.xlabel('Value of K for KNN')
plt.ylabel('Cross-Validated Accuracy')
plt.show()

model_knn = KNeighborsClassifier(n_neighbors=3)
model_knn.fit(X_train,Y_train)

Y_pred  = model_knn.predict(X_test)

print(classification_report(Y_test, Y_pred))

model_knn = KNeighborsClassifier(n_neighbors=3)
model_knn.fit(X_train,Y_train)

Y_pred  = model_knn.predict(X_test)

print(classification_report(Y_test, Y_pred))

"""# Random Forest Model"""

#Import Random Forest Model
from sklearn.ensemble import RandomForestClassifier

#Create a Gaussian Classifier
clf_RFC=RandomForestClassifier(n_estimators=100)

#Train the model using the training sets y_pred=clf.predict(X_test)
clf_RFC.fit(X_train,Y_train)

Y_pred=clf_RFC.predict(X_test)

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

# Model Accuracy, how often is the classifier correct?
kfold = KFold(n_splits=10, random_state=7, shuffle=True)
results = cross_val_score(clf_RFC, X_train,Y_train , cv=kfold)
print(results.mean())

print(results)

print(classification_report(Y_test, Y_pred))

import pickle
pickle_out = open("clf_RFC.pkl","wb")
pickle.dump(clf_RFC,pickle_out)
pickle_out.close()